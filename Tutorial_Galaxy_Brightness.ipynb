{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for brightness profile fitting\n",
    "\n",
    "http://adsabs.harvard.edu/abs/2018MNRAS.475..894T\n",
    "\n",
    "The half light radius ($R_{e}$) of a galaxy is the radius at which half of the total light of the system is emitted. This assumes the galaxy has either intrinsic spherical symmetry or is at least circularly symmetric as viewed in the plane of the sky.\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/3c/Half_light_radius_simple.svg\" style=\"width: 200px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['KERAS_BACKEND'] = 'theano'\n",
    "#os.environ['THEANO_FLAGS'] = \"device=cuda,force_device=True,floatX=float32\"\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import MaxPooling2D, Convolution2D, Conv2D\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET PATH \n",
    "Set the path to the folder containing the simulated input datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where you take the simulated data for train/validation\n",
    "pathinData = '/home/jovyan/InfieriSchool/DeeLegato_1c/' \n",
    "#where do you wanna save the results\n",
    "pathinModel = '/home/jovyan/InfieriSchool/DeeLegato_1c/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Section 1: Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've saved the data that you will use in this tutorial in numpy format. The data consist of 30,000 stamps of simulated HST/CANDELS galaxies (the design matrix X) and the correspondent half light radius, i.e. the parameter that we aim to predict (stored in the target file Y). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(pathinData+'StampsSimulated_tutorial.npy')\n",
    "Y = np.load(pathinData+'ParametersSimulated_tutorial.npy') \n",
    "\n",
    "\n",
    "#visualize the data\n",
    "i = 200\n",
    "plt.imshow(X[i,0,:,:,],clim=(0,.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scale the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================== \n",
    "# Right shape\n",
    "#===============================================\n",
    "print ('X.shape= ', X.shape)\n",
    "X = np.expand_dims(X[:,0,:,:], axis=3)\n",
    "print ('X.shape= ', X.shape)\n",
    "\n",
    "print ('Y.shape= ', Y.shape)\n",
    "Y = Y.reshape(-1,1)\n",
    "print ('Y.shape= ', Y.shape)\n",
    "\n",
    "#=============================================== \n",
    "# Scale (normalize by mean and by standard deviation)\n",
    "#===============================================\n",
    "scaler = preprocessing.StandardScaler().fit(Y)\n",
    "Y=scaler.transform(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split into training and test sets\n",
    "\n",
    "You should define here your dataset for training and for validation. Let's start taking just 4/5 for the training and 1/5 for the validation. If you want you can play with this division to see if you can improve things further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting in Training and Test datasets\n",
    "print(\"Total number of images: \", len(X))\n",
    "X_train = X[0:len(X)//5*4,:,:,:]\n",
    "print(\"Total number of images for the training: \", len(X_train))\n",
    "X_val = X[len(X)//5*4:,:,:,:]\n",
    "print(\"Total number of images for the validation: \", len(X_val))\n",
    "\n",
    "print(\"Total number of labels: \", len(Y))\n",
    "\n",
    "Y_train = Y[0:len(Y)//5*4,0]\n",
    "print(\"Total number of labels for the training: \", len(Y_train))\n",
    "\n",
    "Y_val = Y[len(Y)//5*4:,0]\n",
    "print(\"Total number of labels for the validation: \", len(Y_val))\n",
    "\n",
    "print ('Y_train.shape= ', Y_train.shape)\n",
    "print ('Y_val.shape= ', Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Built a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model():\n",
    "    ## PARAMETERS OF THE MODEL\n",
    "    print ('Building the model')\n",
    "    dropoutpar = 0.15\n",
    "    img_rows=128\n",
    "    img_cols=128\n",
    "    img_channels=1\n",
    "    depth=16\n",
    "    nb_dense = 64  \n",
    "\n",
    "\n",
    "    # KERAS SEQUENTIAL-MODEL\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(depth, (4, 4),activation='relu', input_shape=(img_rows, img_cols, img_channels), padding=\"same\"))\n",
    "    model.add(Conv2D(depth, (4, 4),activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dropoutpar))\n",
    "\n",
    "    model.add(GaussianNoise(0.01,input_shape=( img_rows, img_cols,img_channels)))\n",
    "    model.add(Conv2D(4*depth, (3, 3),activation='relu', padding=\"same\"))\n",
    "    model.add(Conv2D(4*depth, (3, 3),activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(4*depth, (2, 2),activation='relu', padding=\"same\"))\n",
    "    model.add(Conv2D(4*depth, (2, 2),activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2*nb_dense))\n",
    "    model.add(Dense(nb_dense))\n",
    "    model.add(Dense(1))\n",
    "    print(model.summary())\n",
    "    print(\"The number of layers in the model is: \" ,len(model.layers))\n",
    "    return model\n",
    "\n",
    "Build_Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Built a training module\n",
    "\n",
    "Let's TRAIN the model using SGD + momentum  ===> you can try different loss, optimizer and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fit_Model(X_train, X_val,  Y_train, Y_val,  model,verbose):\n",
    "\n",
    "    lr=0.01\n",
    "    decay=0\n",
    "    momentum=0.9\n",
    "    sgd = SGD(lr=lr, decay=decay, momentum=momentum, nesterov=True)\n",
    "    \n",
    "    model.compile(loss='mean_absolute_error', optimizer=sgd)\n",
    "\n",
    "    #Do you wanna use data augmentation?\n",
    "    data_augmentation = False\n",
    "    #hyperparameters of the training. Try different\n",
    "    batch_size = 64\n",
    "    nb_epoch = 5\n",
    "\n",
    "    if data_augmentation == False:\n",
    "        print('Not using data augmentation.')\n",
    "        history = model.fit(X_train, Y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=nb_epoch,\n",
    "                  validation_data=(X_val, Y_val),\n",
    "                  shuffle=True,\n",
    "                  verbose=verbose)\n",
    "    if data_augmentation == True:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # this will do preprocessing and realtime data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                width_shift_range=0.0,  # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.0,  # randomly shift images vertically (fraction of total height)\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=True)  # randomly flip images\n",
    "\n",
    "        # compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied)\n",
    "\n",
    "        datagen.fit(X_train)\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=10), ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "\n",
    "        # fit the model on the batches generated by datagen.flow()\n",
    "        history = model.fit_generator(datagen.flow(X_train, Y_train,\n",
    "                                batch_size=batch_size),\n",
    "                                samples_per_epoch=X_train.shape[0],\n",
    "                                callbacks=callbacks,\n",
    "                                epochs=nb_epoch,\n",
    "                                validation_data=(X_val, Y_val),\n",
    "                                verbose=verbose)\n",
    "\n",
    "        return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILT model\n",
    "model = Build_Model()\n",
    "\n",
    "#train model\n",
    "model, history = Fit_Model(X_train, X_val, Y_train, Y_val, model, verbose=2)\n",
    "\n",
    "# do you wanna save the model?\n",
    "saveModel = True\n",
    "if saveModel == True:\n",
    "    #save scaler\n",
    "    scalerfile = pathinModel+'scaler.sav'\n",
    "    pickle.dump(scaler, open(scalerfile, 'wb'))\n",
    "    model.save(pathinModel+'model.h5')\n",
    "    print(\"Save model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the performance of your DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot learning curves, look and the training error, validation error and the generalization error. Do we observe overfitting, underfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.plot(history.epoch, history.history['loss'], label='loss')\n",
    "plt.plot(history.epoch, history.history['val_loss'], label='val_loss')\n",
    "plt.title('Training performance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('foo.png')\n",
    "\n",
    "print(\"Best validation loss: %.3f\" % (np.min(history.history['val_loss'])))\n",
    "print(\"at: %d\" % np.argmin(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. A better metric - $R^2$\n",
    "The choice of the metric to evaluate the performance of our ML is a fundamental step. The choice of the metric depend on the particular task that we aim to solve.  Here we chose the coefficient of regression $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = scaler.inverse_transform(Y_val)\n",
    "pred = scaler.inverse_transform(model.predict(X_val))\n",
    "pred = pred[:,0]\n",
    "mse=np.mean(np.square(pred-val))\n",
    "R2 = 1. - mse/np.square(np.std(val))\n",
    "print ('R2=', R2)\n",
    "\n",
    "\n",
    "#and now we plot prediction versus validation value\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.title('Val sample. $R^2$ = %s' % (R2), size=11)\n",
    "plt.xlabel('Par', fontsize=13)\n",
    "plt.ylabel(\"Par Predicted\", fontsize=13)\n",
    "plt.scatter(val, pred)\n",
    "plt.savefig('scatter.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What's the best you can get?\n",
    "Optimize the model as much as you can. Try to modify the model and hyperparameters. Ensure you always keep aside a test set to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on real data\n",
    "\n",
    "Now try yourself to test on real: \n",
    "\n",
    "1. Load the real data \n",
    "2. Plot some of the images to see how real data looks like\n",
    "3. Test the model as it is on real data (hint: model.evaluate())\n",
    "4. Load model and keep training (transfer learning) using part of the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "trained_model=load_model('best_model.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
