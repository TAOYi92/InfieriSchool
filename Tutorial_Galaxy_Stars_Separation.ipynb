{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Star-Galaxy separation\n",
    "https://arxiv.org/pdf/1608.04369.pdf\n",
    "\n",
    "\n",
    "Most existing star-galaxy classifiers require careful feature extraction and selection. The latest advances in machine learning that use deep convolutional neural networks allow a machine to automatically learn the features directly from data, minimizing the need for input from human experts. In these lab we present a star-galaxy classification framework that uses deep convolutional neural networks to solve this problem\n",
    "\n",
    "<img src=\"https://old.ipac.caltech.edu/2mass/releases/spr99/doc/test/jarrett2/old/star_gal/jhk_lowdensity.gif\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout,  Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "#from keras.layers.noise import GaussianNoise\n",
    "#from keras.models import load_model\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "#from keras.callbacks import EarlyStopping\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "#from keras.utils import np_utils\n",
    "from keras.constraints import maxnorm\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET PATH\n",
    "Set the path to the folder containing the inputs datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/diego/catalogs/dataNumpyForm/SDSS/StarGalaxy_Images1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a38bccd7ceaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpathinData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/diego/catalogs/dataNumpyForm/SDSS/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_tot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathinData\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'StarGalaxy_Images1.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mY_cat\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathinData\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'StarGalaxy_pandas1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/diego/catalogs/dataNumpyForm/SDSS/StarGalaxy_Images1.npy'"
     ]
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# (0.1) CHOOSE SETTING\n",
    "#=====================================================================\n",
    "#pathinData = '/home/diego/catalogs/dataNumpyForm/'\n",
    "#Y_sim = np.load(pathinData+'ParametersSimulated_ALL.npy')\n",
    "#X_sim = np.load(pathinData+'SimulatedStamps_ALL.npy')\n",
    "\n",
    "pathinData = '/home/diego/catalogs/dataNumpyForm/SDSS/'\n",
    "X_tot = np.load(pathinData+'StarGalaxy_Images1.npy')\n",
    "Y_cat =  pd.read_pickle(pathinData+'StarGalaxy_pandas1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Load Data\n",
    "\n",
    "I've saved the data that you will use in this tutorial in numpy format. The data consist of ...., The class that we aim to predict is stored in the target file Y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_cat.loc[Y_cat['class'] == 'STAR', 'class'] = 1.\n",
    "Y_cat.loc[Y_cat['class'] == 'QSO', 'class'] = 1.\n",
    "Y_cat.loc[Y_cat['class'] == 'GALAXY', 'class'] = 0.\n",
    "\n",
    "\n",
    "\n",
    "x = Y_cat.as_matrix(['class'])\n",
    "\n",
    "\n",
    "\n",
    "Y_tot = []\n",
    "for i in range (len(x)):\n",
    "    if x[i] == 0:\n",
    "        Y_tot = np.append(Y_tot,0.)\n",
    "    else:\n",
    "        Y_tot = np.append(Y_tot,1.)\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "X_tot = np.moveaxis(X_tot, 3, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting in Training and Test datasets\n",
    "X_train = X_tot[0:len(X_tot)//5*4,:,:,:]   \n",
    "Y_train = Y_tot[0:len(Y_tot)//5*4,]\n",
    "X_val = X_tot[len(X_tot)//5*4:,:,:,:]\n",
    "Y_val = Y_tot[len(Y_tot)//5*4:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Build the CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================== \n",
    "# (3) MODEL MODULEs\n",
    "#=====================================================================\n",
    "#=========================================================================  \n",
    "#BUILT Model\n",
    "#================================================ \n",
    "#======================\n",
    "\n",
    "def Model1():\n",
    "    #Parameter Network\n",
    "    img_rows=400\n",
    "    img_cols=400\n",
    "    dropoutpar=0.15\n",
    "    img_channels=3\n",
    "    depth=16\n",
    "    #model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(depth, 3, 3,init='normal',activation='relu', W_constraint=maxnorm(3),border_mode='same', input_shape=(img_channels,img_rows, img_cols)))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Convolution2D(depth*2, 3, 3,activation='relu', W_constraint=maxnorm(3),border_mode='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Convolution2D(depth*4, 3, 3,activation='relu', W_constraint=maxnorm(3),border_mode='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(dropoutpar)) \n",
    "    model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def Model2():\n",
    "    img_rows=128\n",
    "    img_cols=128\n",
    "    img_channels=1\n",
    "    depth=16\n",
    "    dropoutpar=0.15\n",
    "    #model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(depth, 3, 3,init='normal',activation='relu', W_constraint=maxnorm(3),border_mode='same', input_shape=(img_channels, img_rows, img_cols)))\n",
    "    model.add(Convolution2D(depth*2, 3, 3,activation='relu', W_constraint=maxnorm(3),border_mode='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "    model.add(Convolution2D(depth*4, 3, 3,activation='relu', W_constraint=maxnorm(3),border_mode='same'))\n",
    "    model.add(Convolution2D(depth*4, 3, 3,activation='relu', W_constraint=maxnorm(3),border_mode='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "    model.add(Convolution2D(depth*4, 3, 3,activation='relu', W_constraint=maxnorm(3),border_mode='same'))\n",
    "    model.add(Convolution2D(depth*4, 3, 3,activation='relu', W_constraint=maxnorm(3),border_mode='same'))\n",
    "    model.add(Convolution2D(depth*4, 3, 3,activation='relu', W_constraint=maxnorm(3),border_mode='same'))\n",
    "    model.add(Convolution2D(depth*4, 3, 3,activation='relu', W_constraint=maxnorm(3),border_mode='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(dropoutpar))\n",
    "    model.add(Dense(32, activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Build a training module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================\n",
    "# FIT MODEL\n",
    "#==================\n",
    "def Fit_Model(X_train, X_val,  Y_train, Y_val,  model):   \n",
    "    batch_size=64\n",
    "    nb_epoch = 4    \n",
    "    lr=0.01 \n",
    "    decay=0   \n",
    "    momentum=0.9 \n",
    "    \n",
    "    sgd = SGD(lr=lr, decay=decay, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "  \n",
    "        \n",
    "    #data argumentation    \n",
    "    datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,\n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,\n",
    "            samplewise_std_normalization=False,\n",
    "            zca_whitening=False,\n",
    "            rotation_range = 0., #360\n",
    "            width_shift_range=0.05, #0.05  \n",
    "            height_shift_range=0.05, #0.05 \n",
    "            horizontal_flip=False,\n",
    "            vertical_flip=False,\n",
    " \t    zoom_range=0.) #[0.7,1.3] \n",
    "    \n",
    "    #Initialize variables\n",
    "    train_loss_vec=[]\n",
    "    test_loss_vec=[]\n",
    "    maxPatience = 10\n",
    "    consumedPatience =0\n",
    "    test_loss = 999.\n",
    "    best_loss = 999.\n",
    "    epochsDone = 0\n",
    "    bestModel=[]                             \n",
    "                                                                  \n",
    "    for e in range(1, nb_epoch):\n",
    "        epochsDone = epochsDone+1\n",
    "        train_loss = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=1)\n",
    "        pred=model.predict_classes(X_val)\n",
    "        test_loss = 1./accuracy_score(Y_val, pred)           \n",
    "        #train_loss_vec = np.append(train_loss_vec,train_loss.history['loss']) \n",
    "        #test_loss_vec = np.append(test_loss_vec,test_loss)\n",
    "                \n",
    "        #implementation early stopping\n",
    "        print ('Validation loss = ', test_loss)\n",
    "        print (' ')         \n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            bestModel = model\n",
    "            consumedPatience = 0\n",
    "            print ('Here I save best model and best loss')\n",
    "        else:            \n",
    "            consumedPatience = consumedPatience + 1\n",
    "        if consumedPatience == maxPatience:\n",
    "           break  \n",
    "              \n",
    "    return bestModel, epochsDone      \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Train and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Model1()\n",
    "model.summary()\n",
    "print('-'*30)\n",
    "print('Fitting model...')\n",
    "print('-'*30)\n",
    "model, epochsDone = Fit_Model(X_train, X_val, Y_train, Y_val, model)  \n",
    "#then SAVE the MODEL \n",
    "model.save(pathinModel+'Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict_classes(X_val)\n",
    "print(classification_report(pred,Y_val))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(Y_val, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notas: Early Stopping, Best model, qué es lo mejor que puedo esperar?, pedirles que jueguen con los parametros, darles algunas imagenes de test para probar?, por qué dos modelos? les damos solo uno y pistas para el segundo?, Por qué es importante el problema?, darles el summary y que hagan ellos la arquitectura\n",
    " Necesitamos el train_loss_vec y demás?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
